{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechCrunchScraper:\n",
    "    MODE = 'normal' # or headless\n",
    "    BLOCK_IMAGES = True\n",
    "    BLOCK_JS = False\n",
    "    DRIVER_PATH = '/Users/hadi/Documents/workspace/daneshkar/week 11 (project sraping)/shared/selenium/chromedriver'\n",
    "    WEBSITE_MAIN_PAGE = 'https://techcrunch.com'\n",
    "    HTML_FOLDER_PATH = '/Users/hadi/Documents/workspace/daneshkar/week 11 (project sraping)/scraped_html'\n",
    "\n",
    "    def __init__(self, mode='normal', block_images=True, block_js=False) -> None:\n",
    "        self.MODE = mode\n",
    "        self.BLOCK_IMAGES = block_images\n",
    "        self.BLOCK_JS = block_js\n",
    "\n",
    "        self.driver = None\n",
    "        self.service = None\n",
    "        self.all_categories_list = None\n",
    "\n",
    "    def run_driver(self, page_load_timeout=10):\n",
    "        if self.driver is not None:\n",
    "            return\n",
    "        if self.MODE == 'headless':\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.headless = True\n",
    "            options.add_argument(\"--window-size=1920,1200\")\n",
    "        elif self.MODE == 'normal':\n",
    "            options = webdriver.ChromeOptions()\n",
    "\n",
    "        if self.BLOCK_IMAGES or self.BLOCK_JS:\n",
    "            ### This blocks images and javascript requests\n",
    "            block_dict = {}\n",
    "            if self.BLOCK_IMAGES:\n",
    "                block_dict['images'] = 2\n",
    "            if self.BLOCK_JS:\n",
    "                block_dict['javascript'] = 2\n",
    "            chrome_prefs = {\n",
    "                \"profile.default_content_setting_values\": block_dict\n",
    "            }\n",
    "            options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "\n",
    "        self.service = Service(executable_path=self.DRIVER_PATH)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=options)\n",
    "        self.driver.set_page_load_timeout(page_load_timeout)\n",
    "        return self.driver\n",
    "    \n",
    "    def open_link_in_driver(self, link, try_again_if_timeout=True):\n",
    "        if self.driver is None:\n",
    "            self.run_driver()\n",
    "        while True:\n",
    "            try:\n",
    "                self.driver.get(link)\n",
    "            except TimeoutException:\n",
    "                if try_again_if_timeout:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    def scroll_to_bottom(self):\n",
    "        javaScript = \"window.scrollBy(0, 100000);\"\n",
    "        self.driver.execute_script(javaScript)\n",
    "\n",
    "    def get_list_of_all_categories(self):\n",
    "        if self.all_categories_list is not None:\n",
    "            return self.all_categories_list\n",
    "        \n",
    "        self.open_link_in_driver(self.WEBSITE_MAIN_PAGE)\n",
    "\n",
    "        categories_links_path = '//header[contains(@class, \"site-navigation\")]//ul[contains(@class, \"menu\")]/li[@class=\"menu__item\"]/a'\n",
    "        categories_links = self.driver.find_elements(By.XPATH, categories_links_path)\n",
    "        main_categories_list = [(link.get_attribute('href'), link.text) for link in categories_links if '/category/' in link.get_attribute('href')]\n",
    "\n",
    "        more_categories_btn = '//header[contains(@class, \"site-navigation\")]//ul[contains(@class, \"menu\")]/li[@class=\"menu__item more-link\"]/a'\n",
    "        more_btn = self.driver.find_element(By.XPATH, more_categories_btn)\n",
    "        more_btn.click()\n",
    "\n",
    "        more_categories_links_path = '//header[contains(@class, \"site-navigation\")]//div[@class=\"desktop-nav navigation-desktop__flyout\"]//li[@class=\"menu__item\"]/a'\n",
    "        more_categories_links = self.driver.find_elements(By.XPATH, more_categories_links_path)\n",
    "        more_categories_list = [(link.get_attribute('href'), link.text) for link in more_categories_links if '/category/' in link.get_attribute('href')]\n",
    "\n",
    "        all_categories_list = main_categories_list + more_categories_list\n",
    "        self.all_categories_list = all_categories_list\n",
    "        return self.all_categories_list\n",
    "    \n",
    "    def get_article_data_from_html(self, article_header):\n",
    "        def get_article_header_type():\n",
    "            try:\n",
    "                article_category = article_header.find_element(By.XPATH, './div[@class=\"article__primary-category\"]/a')\n",
    "                return {\n",
    "                    'type': types['article_category'], \n",
    "                    'text': article_category.text, \n",
    "                    'href': article_category.get_attribute('href'),\n",
    "                }\n",
    "            except:\n",
    "                try:\n",
    "                    article_label = article_header.find_element(By.XPATH, './div[@class=\"featured-article__label\"]/div[contains(@class, \"featured-article__label__text\")]')\n",
    "                    return {\n",
    "                        'type': types['article_label'], \n",
    "                        'text': article_label.text, \n",
    "                        'href': article_label.get_attribute('href'),\n",
    "                    }\n",
    "                except:\n",
    "                    article_event_title = article_header.find_element(By.XPATH, './h3[@class=\"article__event-title\"]/a')\n",
    "                    return {\n",
    "                        'type': types['article_event'], \n",
    "                        'text': article_event_title.text, \n",
    "                        'href': article_event_title.get_attribute('href'),\n",
    "                    }\n",
    "                \n",
    "        types = {'article_category': 'Category', 'article_label': 'Label', 'article_event': 'Event'}\n",
    "        title = article_header.find_element(By.XPATH, './h2[@class=\"post-block__title\"]').text\n",
    "        # //div[contains(@class, \"river\")]/div//article/header//div[@class=\"post-block__meta\"]//span[@class=\"river-byline__authors\"]//a\n",
    "        author_name_el = article_header.find_element(By.XPATH, './/div[@class=\"post-block__meta\"]//span[@class=\"river-byline__authors\"]//a')\n",
    "        author_name, author_link = author_name_el.text, author_name_el.get_attribute('href')\n",
    "        # //div[contains(@class, \"river\")]/div//article/header//div[@class=\"post-block__meta\"]//span[@class=\"river-byline__full-date-time__wrapper\"]//time\n",
    "        date_and_time = article_header.find_element(By.XPATH, './/div[@class=\"post-block__meta\"]//div[@class=\"river-byline__full-date-time__wrapper\"]//time').get_attribute('datetime')\n",
    "        article_canonical_link = article_header.find_element(By.XPATH, './h2[@class=\"post-block__title\"]/a').get_attribute('href')\n",
    "        return {\n",
    "            'title': title,\n",
    "            'article_link': article_canonical_link,\n",
    "            'header': get_article_header_type(),\n",
    "            'author_name': author_name,\n",
    "            'author_link': author_link,\n",
    "            'date_and_time': date_and_time,\n",
    "        }\n",
    "    \n",
    "    def scrape_new_articles_of_category_link(self, category_page_link, already_scraped_articles_num=0):\n",
    "        if already_scraped_articles_num == 0:\n",
    "            self.open_link_in_driver(category_page_link)\n",
    "        else:\n",
    "            self.scroll_to_bottom()\n",
    "        category_river_div_path = '//div[contains(@class, \"river\")]/div'\n",
    "        river_div = self.driver.find_element(By.XPATH, category_river_div_path)\n",
    "        articles_elements = river_div.find_elements(By.XPATH, '//article')\n",
    "        articles_elements = articles_elements[already_scraped_articles_num:]\n",
    "        for article in articles_elements:\n",
    "            article_header = article.find_element(By.XPATH, './header')\n",
    "            self.save_article_data_in_database(self.get_article_data_from_html(article_header))\n",
    "\n",
    "    def click_load_more_in_category_page(self, wait_after=10, try_until_success=True):\n",
    "        while True:\n",
    "            try:\n",
    "                time.sleep(wait_after)\n",
    "                load_more_btn_xpath = '//*[@id=\"tc-main-content\"]//button[contains(@class, \"load-more\")]'\n",
    "                load_more_btn = self.driver.find_element(By.XPATH, load_more_btn_xpath)\n",
    "                load_more_btn.click()\n",
    "                return\n",
    "            except:\n",
    "                if try_until_success:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "    def get_number_of_current_articles_in_page(self):\n",
    "        category_river_div_path = '//div[contains(@class, \"river\")]/div'\n",
    "        river_div = self.driver.find_element(By.XPATH, category_river_div_path)\n",
    "        articles_elements = river_div.find_elements(By.XPATH, '//article')\n",
    "        return len(articles_elements)\n",
    "\n",
    "    def scrape_category_scroll_down(self, category_link):\n",
    "        already_scraped_articles_num = 0\n",
    "        while True:\n",
    "            self.scrape_new_articles_of_category_link(category_link, already_scraped_articles_num)\n",
    "            already_scraped_articles_num = self.get_number_of_current_articles_in_page()\n",
    "            print(already_scraped_articles_num)\n",
    "            self.scroll_to_bottom()\n",
    "            self.click_load_more_in_category_page(try_until_success=True)\n",
    "\n",
    "    def save_article_data_in_database(self, article_data):\n",
    "        # {\n",
    "        #     'title': title,\n",
    "        #     'article_link': article_canonical_link,\n",
    "        #     'header': get_article_header_type(),get_number_of_current_articles_in_page\n",
    "        #     'author_name': author_name,\n",
    "        #     'author_link': author_link,\n",
    "        #     'date_and_time': date_and_time,\n",
    "        # }\n",
    "        from app_database import save_article_data_to_database\n",
    "        save_article_data_to_database(\n",
    "            article_data['article_link'],\n",
    "            article_data['author_name'],\n",
    "            article_data['author_link'],\n",
    "            article_data['header']['href'],\n",
    "            article_data['title'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
